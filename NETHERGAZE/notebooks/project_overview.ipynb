{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NETHERGAZE: Markerless Augmented Reality Pipeline\n",
        "\n",
        "**Author:** Milan Savard  \n",
        "**Course:** CS366 F25 Final Personal Project  \n",
        "**Date:** November 2025\n",
        "\n",
        "---\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "NETHERGAZE is a markerless augmented reality (AR) system built using computer vision techniques. Unlike traditional AR systems that rely on fiducial markers (like ArUco or AprilTags), NETHERGAZE uses **natural feature tracking** to estimate camera pose and overlay virtual content.\n",
        "\n",
        "### Key Technologies\n",
        "- **OpenCV** for image processing and computer vision\n",
        "- **ORB (Oriented FAST and Rotated BRIEF)** for feature detection and description\n",
        "- **Lucas-Kanade Optical Flow** for frame-to-frame feature tracking\n",
        "- **Essential Matrix + RANSAC** for robust pose estimation\n",
        "- **Temporal filtering** for smooth AR overlays\n",
        "- **Sparse SLAM** for 3D mapping and loop closure\n",
        "- **Depth-aware rendering** for AR occlusion handling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. [Architecture Overview](#1-architecture-overview)\n",
        "2. [Implementation Progress](#2-implementation-progress)\n",
        "3. [Component Deep Dive](#3-component-deep-dive)\n",
        "4. [Demo: Running the Pipeline](#4-demo-running-the-pipeline)\n",
        "5. [Configuration Options](#5-configuration-options)\n",
        "6. [Next Steps & Roadmap](#6-next-steps--roadmap)\n",
        "7. [References](#7-references)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. Architecture Overview\n",
        "\n",
        "The NETHERGAZE pipeline consists of seven main stages:\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ   Capture   ‚îÇ -> ‚îÇ   Tracking   ‚îÇ -> ‚îÇ Pose Estimation‚îÇ -> ‚îÇ  Mapping  ‚îÇ -> ‚îÇ   Overlay   ‚îÇ -> ‚îÇ Display ‚îÇ\n",
        "‚îÇ  (video.py) ‚îÇ    ‚îÇ (feature.py) ‚îÇ    ‚îÇ   (pose.py)    ‚îÇ    ‚îÇ(mapping.py‚îÇ    ‚îÇ (overlay.py)‚îÇ    ‚îÇ (ui.py) ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                                                                   ‚îÇ                  ‚îÇ\n",
        "                                                                   v                  v\n",
        "                                                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "                                                            ‚îÇ    Loop     ‚îÇ    ‚îÇ  Occlusion  ‚îÇ\n",
        "                                                            ‚îÇ   Closure   ‚îÇ    ‚îÇ(occlusion.py‚îÇ\n",
        "                                                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "### Module Responsibilities\n",
        "\n",
        "| Module | File | Purpose |\n",
        "|--------|------|---------|  \n",
        "| **VideoProcessor** | `src/video.py` | Camera capture, preprocessing, backend selection |\n",
        "| **FeatureTracker** | `src/tracking/feature.py` | ORB detection, optical flow, keyframe management |\n",
        "| **PoseEstimator** | `src/pose.py` | Essential matrix, pose recovery, temporal filtering |\n",
        "| **SparseMap** | `src/mapping.py` | 3D point cloud, keyframe management, loop closure |\n",
        "| **OcclusionHandler** | `src/occlusion.py` | Depth estimation, occlusion masks, depth-aware rendering |\n",
        "| **OverlayRenderer** | `src/overlay.py` | 2D/3D overlay rendering, blending |\n",
        "| **UserInterface** | `src/ui.py` | Display window, keyboard controls |\n",
        "| **NETHERGAZEApp** | `src/main.py` | Pipeline orchestration, CLI interface |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Implementation Progress\n",
        "\n",
        "### ‚úÖ Completed Components\n",
        "\n",
        "| Component | Status | Description |\n",
        "|-----------|--------|-------------|\n",
        "| Video Capture | ‚úÖ Complete | Multi-backend support (AVFoundation, DirectShow, V4L2) |\n",
        "| Feature Tracking | ‚úÖ Complete | ORB + optical flow with keyframe management |\n",
        "| Pose Estimation | ‚úÖ Complete | Essential matrix decomposition with temporal smoothing |\n",
        "| Overlay Rendering | ‚úÖ Complete | 2D primitives + 3D wireframes |\n",
        "| User Interface | ‚úÖ Complete | OpenCV window with keyboard controls |\n",
        "| Pipeline Orchestration | ‚úÖ Complete | CLI-driven main application |\n",
        "| Configuration System | ‚úÖ Complete | JSON config with sensible defaults |\n",
        "| Camera Calibration Tool | ‚úÖ Complete | Interactive chessboard calibration with preview mode |\n",
        "| Integration Tests | ‚úÖ Complete | Synthetic video testing, benchmarks, detector comparison |\n",
        "| SLAM/Mapping | ‚úÖ Complete | Sparse 3D mapping with keyframes, triangulation, loop closure |\n",
        "| Occlusion Handling | ‚úÖ Complete | Depth estimation, occlusion masks, depth-aware rendering |\n",
        "\n",
        "### üöß In Progress / Planned\n",
        "\n",
        "| Component | Status | Priority |\n",
        "|-----------|--------|----------|\n",
        "| Scale Recovery | üöß Partial | High |\n",
        "| Textured 3D Models | üöß Planned | Medium |\n",
        "| Dense Depth (MiDaS) | üöß Future | Low |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Component Deep Dive\n",
        "\n",
        "Let's explore each component in detail.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /Users/milansavard/Desktop/GitHub/ComputerVision/NETHERGAZE\n",
            "Source path: /Users/milansavard/Desktop/GitHub/ComputerVision/NETHERGAZE/src\n"
          ]
        }
      ],
      "source": [
        "# Setup: Add src to path for imports\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Navigate to project root\n",
        "notebook_dir = os.getcwd()\n",
        "project_root = os.path.dirname(notebook_dir) if 'notebooks' in notebook_dir else notebook_dir\n",
        "src_path = os.path.join(project_root, 'src')\n",
        "sys.path.insert(0, src_path)\n",
        "\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Source path: {src_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Feature Tracking\n",
        "\n",
        "The `FeatureTracker` class implements a hybrid approach:\n",
        "1. **Detection**: ORB features are detected when tracking is lost or features drop below threshold\n",
        "2. **Tracking**: Lucas-Kanade optical flow tracks features frame-to-frame (faster than re-detection)\n",
        "3. **Keyframes**: Best frames are stored for re-localization when tracking fails\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature Tracking Configuration:\n",
            "  Method: orb\n",
            "  Max Features: 1000\n",
            "  Use Optical Flow: True\n",
            "  Optical Flow Window: 21\n",
            "  Reacquire Threshold: 300\n",
            "  Keyframe Interval: 15\n",
            "  Max Keyframes: 5\n"
          ]
        }
      ],
      "source": [
        "from tracking.feature import FeatureTracker, TrackingConfiguration\n",
        "\n",
        "# View default configuration\n",
        "default_config = TrackingConfiguration()\n",
        "print(\"Feature Tracking Configuration:\")\n",
        "print(f\"  Method: {default_config.method}\")\n",
        "print(f\"  Max Features: {default_config.max_features}\")\n",
        "print(f\"  Use Optical Flow: {default_config.use_optical_flow}\")\n",
        "print(f\"  Optical Flow Window: {default_config.optical_flow_win_size}\")\n",
        "print(f\"  Reacquire Threshold: {default_config.reacquire_threshold}\")\n",
        "print(f\"  Keyframe Interval: {default_config.keyframe_interval}\")\n",
        "print(f\"  Max Keyframes: {default_config.max_keyframes}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Pose Estimation\n",
        "\n",
        "The `PoseEstimator` class recovers camera motion from 2D-2D correspondences:\n",
        "\n",
        "1. **Essential Matrix**: Computed from matched points using RANSAC\n",
        "2. **Pose Recovery**: Decompose E into rotation (R) and translation (t)\n",
        "3. **Temporal Filtering**: EMA smoothing + outlier rejection for stable overlays\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pose Filter Configuration:\n",
            "  Enable Smoothing: True\n",
            "  Smoothing Alpha (EMA): 0.3\n",
            "  Enable Outlier Rejection: True\n",
            "  Max Translation Jump: 0.5 m\n",
            "  Max Rotation Jump: 0.5 rad\n",
            "  Min Inliers Threshold: 10\n"
          ]
        }
      ],
      "source": [
        "from pose import PoseEstimator, PoseFilterConfig\n",
        "\n",
        "# View pose filter configuration\n",
        "filter_config = PoseFilterConfig()\n",
        "print(\"Pose Filter Configuration:\")\n",
        "print(f\"  Enable Smoothing: {filter_config.enable_smoothing}\")\n",
        "print(f\"  Smoothing Alpha (EMA): {filter_config.smoothing_alpha}\")\n",
        "print(f\"  Enable Outlier Rejection: {filter_config.enable_outlier_rejection}\")\n",
        "print(f\"  Max Translation Jump: {filter_config.max_translation_jump} m\")\n",
        "print(f\"  Max Rotation Jump: {filter_config.max_rotation_jump} rad\")\n",
        "print(f\"  Min Inliers Threshold: {filter_config.min_inliers_threshold}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Overlay Rendering\n",
        "\n",
        "The `OverlayRenderer` supports both 2D screen-space and 3D world-space overlays:\n",
        "\n",
        "**2D Overlays:**\n",
        "- Text labels\n",
        "- Rectangles, circles, lines\n",
        "- Polygons\n",
        "\n",
        "**3D Wireframe Objects:**\n",
        "- Cube, pyramid, grid\n",
        "- Coordinate axes (RGB = XYZ)\n",
        "- Custom wireframes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overlay Configuration:\n",
            "  Enable 2D Overlays: True\n",
            "  Enable 3D Overlays: True\n",
            "  Default 3D Color: (0, 255, 255) (BGR)\n",
            "  Blend Alpha: 0.7\n",
            "  Antialiasing: True\n"
          ]
        }
      ],
      "source": [
        "from overlay import OverlayRenderer, OverlayConfiguration\n",
        "\n",
        "# View overlay configuration\n",
        "overlay_config = OverlayConfiguration()\n",
        "print(\"Overlay Configuration:\")\n",
        "print(f\"  Enable 2D Overlays: {overlay_config.enable_2d_overlays}\")\n",
        "print(f\"  Enable 3D Overlays: {overlay_config.enable_3d_overlays}\")\n",
        "print(f\"  Default 3D Color: {overlay_config.default_3d_color} (BGR)\")\n",
        "print(f\"  Blend Alpha: {overlay_config.blend_alpha}\")\n",
        "print(f\"  Antialiasing: {overlay_config.antialiasing}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Camera Calibration\n",
        "\n",
        "The system uses a camera intrinsic matrix (K) for projection:\n",
        "\n",
        "```\n",
        "K = | fx   0  cx |\n",
        "    |  0  fy  cy |\n",
        "    |  0   0   1 |\n",
        "```\n",
        "\n",
        "Where:\n",
        "- `fx, fy` = focal lengths in pixels\n",
        "- `cx, cy` = principal point (image center)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Default Camera Matrix (K):\n",
            "[[800.   0. 320.]\n",
            " [  0. 800. 240.]\n",
            " [  0.   0.   1.]]\n",
            "\n",
            "Focal Length: fx=800.0, fy=800.0 pixels\n",
            "Principal Point: cx=320.0, cy=240.0 pixels\n",
            "\n",
            "Distortion Coefficients: [0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "from utils import get_config\n",
        "import numpy as np\n",
        "\n",
        "# Load default calibration\n",
        "config = get_config()\n",
        "calib = config['calibration']\n",
        "\n",
        "K = np.array(calib['camera_matrix'])\n",
        "dist = np.array(calib['dist_coeffs'])\n",
        "\n",
        "print(\"Default Camera Matrix (K):\")\n",
        "print(K)\n",
        "print(f\"\\nFocal Length: fx={K[0,0]:.1f}, fy={K[1,1]:.1f} pixels\")\n",
        "print(f\"Principal Point: cx={K[0,2]:.1f}, cy={K[1,2]:.1f} pixels\")\n",
        "print(f\"\\nDistortion Coefficients: {dist}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Demo: Running the Pipeline\n",
        "\n",
        "### Command Line Usage\n",
        "\n",
        "```bash\n",
        "# Basic demo (uses examples/run_demo.py)\n",
        "cd NETHERGAZE\n",
        "python3 examples/run_demo.py\n",
        "\n",
        "# Full pipeline with CLI options\n",
        "python3 src/main.py                    # Default camera\n",
        "python3 src/main.py --camera 1         # Specific camera index\n",
        "python3 src/main.py --video demo.mp4   # Video file input\n",
        "python3 src/main.py --verbose          # Debug logging\n",
        "python3 src/main.py --config cfg.json  # Custom config file\n",
        "```\n",
        "\n",
        "### Keyboard Controls\n",
        "\n",
        "| Key | Action |\n",
        "|-----|--------|\n",
        "| `q` / `ESC` | Quit |\n",
        "| `m` | Toggle feature marker display |\n",
        "| `a` | Toggle pose axes display |\n",
        "| `p` | Pause/Resume |\n",
        "| `h` | Show help |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usage: main.py [-h] [--config CONFIG] [--camera CAMERA] [--video VIDEO]\n",
            "               [--width WIDTH] [--height HEIGHT] [--verbose]\n",
            "\n",
            "NETHERGAZE - Markerless Augmented Reality Pipeline\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --config CONFIG, -c CONFIG\n",
            "                        Path to JSON configuration file\n",
            "  --camera CAMERA, -cam CAMERA\n",
            "                        Camera index to use (default: 0)\n",
            "  --video VIDEO, -v VIDEO\n",
            "                        Path to video file (overrides camera)\n",
            "  --width WIDTH         Video capture width\n",
            "  --height HEIGHT       Video capture height\n",
            "  --verbose, -V         Enable verbose/debug logging\n",
            "\n",
            "Examples:\n",
            "  python main.py                          # Run with default camera\n",
            "  python main.py --camera 1               # Use camera index 1\n",
            "  python main.py --video demo.mp4         # Process video file\n",
            "  python main.py --config my_config.json  # Use custom config\n",
            "  python main.py --verbose                # Enable debug logging\n",
            "        \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display the main.py CLI help\n",
        "import subprocess\n",
        "result = subprocess.run(\n",
        "    ['python3', '../src/main.py', '--help'],\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "print(result.stdout)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Configuration Options\n",
        "\n",
        "All parameters can be customized via JSON config file or modified in `src/utils.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full Configuration:\n",
            "{\n",
            "  \"camera_id\": 0,\n",
            "  \"video_width\": 640,\n",
            "  \"video_height\": 480,\n",
            "  \"video_fps\": 30,\n",
            "  \"camera_backend_priority\": null,\n",
            "  \"camera_init_attempts\": 10,\n",
            "  \"enable_preprocessing\": true,\n",
            "  \"blur_kernel\": [\n",
            "    5,\n",
            "    5\n",
            "  ],\n",
            "  \"contrast_alpha\": 1.0,\n",
            "  \"brightness_beta\": 0,\n",
            "  \"axis_length\": 0.05,\n",
            "  \"feature_tracking\": {\n",
            "    \"method\": \"orb\",\n",
            "    \"max_features\": 1000,\n",
            "    \"quality_level\": 0.01,\n",
            "    \"min_distance\": 7.0,\n",
            "    \"fast_threshold\": 20,\n",
            "    \"orb_scale_factor\": 1.2,\n",
            "    \"orb_nlevels\": 8,\n",
            "    \"akaze_threshold\": 0.001,\n",
            "    \"use_optical_flow\": true,\n",
            "    \"optical_flow_win_size\": 21,\n",
            "    \"optical_flow_max_level\": 3,\n",
            "    \"optical_flow_criteria_eps\": 0.03,\n",
            "    \"optical_flow_criteria_count\": 30,\n",
            "    \"adaptive_optical_flow\": true,\n",
            "    \"reacquire_threshold\": 200,\n",
            "    \"keyframe_interval\": 15,\n",
            "    \"max_keyframes\": 6,\n",
            "    \"min_keyframe_features\": 160,\n",
            "    \"keyframe_quality_threshold\": 0.5,\n",
            "    \"matcher_type\": \"bf_hamming\",\n",
            "    \"match_ratio_threshold\": 0.75,\n",
            "    \"use_grid_detection\": false,\n",
            "    \"grid_rows\": 4,\n",
            "    \"grid_cols\": 4\n",
            "  },\n",
            "  \"calibration\": {\n",
            "    \"calibration_file\": null,\n",
            "    \"camera_matrix\": [\n",
            "      [\n",
            "        800.0,\n",
            "        0.0,\n",
            "        320.0\n",
            "      ],\n",
            "      [\n",
            "        0.0,\n",
            "        800.0,\n",
            "        240.0\n",
            "      ],\n",
            "      [\n",
            "        0.0,\n",
            "        0.0,\n",
            "        1.0\n",
            "      ]\n",
            "    ],\n",
            "    \"dist_coeffs\": [\n",
            "      0.0,\n",
            "      0.0,\n",
            "      0.0,\n",
            "      0.0,\n",
            "      0.0\n",
            "    ]\n",
            "  },\n",
            "  \"pose_filter\": {\n",
            "    \"enable_smoothing\": true,\n",
            "    \"smoothing_alpha\": 0.3,\n",
            "    \"enable_outlier_rejection\": true,\n",
            "    \"max_translation_jump\": 0.5,\n",
            "    \"max_rotation_jump\": 0.5,\n",
            "    \"history_size\": 10,\n",
            "    \"use_median_filter\": false,\n",
            "    \"min_inliers_threshold\": 10\n",
            "  },\n",
            "  \"scale_estimation\": {\n",
            "    \"method\": \"auto\",\n",
            "    \"manual_scale\": 1.0,\n",
            "    \"known_distance\": null,\n",
            "    \"ground_plane_height\": 1.5,\n",
            "    \"reference_object_size\": null,\n",
            "    \"scale_smoothing_alpha\": 0.2,\n",
            "    \"min_scale\": 0.001,\n",
            "    \"max_scale\": 100.0,\n",
            "    \"consistency_threshold\": 0.5\n",
            "  },\n",
            "  \"overlay\": {\n",
            "    \"enable_2d_overlays\": true,\n",
            "    \"enable_3d_overlays\": true,\n",
            "    \"default_3d_color\": [\n",
            "      0,\n",
            "      255,\n",
            "      255\n",
            "    ],\n",
            "    \"default_2d_color\": [\n",
            "      0,\n",
            "      255,\n",
            "      0\n",
            "    ],\n",
            "    \"blend_alpha\": 0.7,\n",
            "    \"antialiasing\": true\n",
            "  },\n",
            "  \"display_width\": 640,\n",
            "  \"display_height\": 480,\n",
            "  \"show_markers\": true,\n",
            "  \"show_axes\": true\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from utils import get_config\n",
        "\n",
        "# Get full configuration\n",
        "config = get_config()\n",
        "\n",
        "# Pretty print\n",
        "print(\"Full Configuration:\")\n",
        "print(json.dumps(config, indent=2, default=str))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Configuration Sections\n",
        "\n",
        "| Section | Description |\n",
        "|---------|-------------|\n",
        "| `feature_tracking` | ORB detection and optical flow parameters |\n",
        "| `calibration` | Camera intrinsic matrix and distortion coefficients |\n",
        "| `pose_filter` | Temporal smoothing and outlier rejection settings |\n",
        "| `overlay` | 2D/3D rendering options |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. Next Steps & Roadmap\n",
        "\n",
        "### ‚úÖ Recently Completed\n",
        "\n",
        "#### 1. Camera Calibration Tool ‚úÖ\n",
        "Fully implemented in `examples/calibrate_camera.py`:\n",
        "- Interactive chessboard capture mode\n",
        "- Batch calibration from image files  \n",
        "- Live undistorted preview\n",
        "- JSON export/import of calibration data\n",
        "\n",
        "```bash\n",
        "# Usage examples\n",
        "python examples/calibrate_camera.py --capture           # Interactive mode\n",
        "python examples/calibrate_camera.py --images *.jpg     # Batch mode\n",
        "python examples/calibrate_camera.py --preview          # Preview calibration\n",
        "```\n",
        "\n",
        "#### 2. Integration Tests ‚úÖ\n",
        "Comprehensive test suite in `tests/test_integration.py`:\n",
        "- Synthetic video generation (checkerboard, feature-rich sequences)\n",
        "- Pipeline metrics collection (tracking rate, pose rate, FPS)\n",
        "- Detector comparison benchmarks (ORB, AKAZE, BRISK)\n",
        "- Video file playback for reproducible testing\n",
        "\n",
        "#### 3. SLAM/Mapping Integration ‚úÖ\n",
        "Sparse SLAM in `src/mapping.py`:\n",
        "- `SparseMap` class with 3D point cloud from triangulated features\n",
        "- Keyframe management with covisibility graph\n",
        "- Loop closure detection with geometric verification\n",
        "- Map persistence (save/load to JSON)\n",
        "\n",
        "#### 4. Occlusion Handling ‚úÖ\n",
        "Depth-aware rendering in `src/occlusion.py`:\n",
        "- `DepthEstimator` - sparse depth from features + ground plane assumption\n",
        "- `OcclusionHandler` - mask generation and compositing\n",
        "- `DepthAwareOverlayRenderer` - proper AR occlusion\n",
        "\n",
        "### üöß Remaining Tasks\n",
        "\n",
        "#### High Priority: Scale Recovery Enhancement\n",
        "Markerless tracking recovers pose up to an unknown scale. Options:\n",
        "- Known object size in scene\n",
        "- IMU integration for metric scale\n",
        "- Stereo camera setup\n",
        "\n",
        "#### Medium Priority: Textured 3D Models\n",
        "Replace wireframes with proper 3D mesh rendering:\n",
        "- Load OBJ/PLY files\n",
        "- OpenGL integration for GPU rendering\n",
        "- Texture mapping\n",
        "\n",
        "#### Low Priority: Dense Depth Integration\n",
        "- MiDaS or similar monocular depth estimation\n",
        "- Better occlusion with dense depth maps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úÖ Camera Calibration Tool - Now Implemented!\n",
        "\n",
        "The calibration tool has been fully implemented. Here's how to use it:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Camera Calibration Skeleton:\n",
            "\n",
            "import cv2\n",
            "import numpy as np\n",
            "import json\n",
            "\n",
            "def calibrate_camera(image_paths, board_size=(9, 6), square_size=0.025):\n",
            "    \"\"\"\n",
            "    Calibrate camera from chessboard images.\n",
            "    \n",
            "    Args:\n",
            "        image_paths: List of paths to calibration images\n",
            "        board_size: (cols, rows) of internal chessboard corners\n",
            "        square_size: Physical size of chessboard square in meters\n",
            "    \n",
            "    Returns:\n",
            "        camera_matrix, dist_coeffs\n",
            "    \"\"\"\n",
            "    # Prepare object points (0,0,0), (1,0,0), (2,0,0), ...\n",
            "    objp = np.zeros((board_size[0] * board_size[1], 3), np.float32)\n",
            "    objp[:, :2] = np.mgrid[0:board_size[0], 0:board_size[1]].T.reshape(-1, 2)\n",
            "    objp *= square_size\n",
            "    \n",
            "    obj_points = []  # 3D points in world\n",
            "    img_points = []  # 2D points in image\n",
            "    \n",
            "    for path in image_paths:\n",
            "        img = cv2.imread(path)\n",
            "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
            "        \n",
            "        ret, corners = cv2.findChessboardCorners(gray, board_size, None)\n",
            "        if ret:\n",
            "            obj_points.append(objp)\n",
            "            corners2 = cv2.cornerSubPix(\n",
            "                gray, corners, (11, 11), (-1, -1),\n",
            "                (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
            "            )\n",
            "            img_points.append(corners2)\n",
            "    \n",
            "    ret, K, dist, rvecs, tvecs = cv2.calibrateCamera(\n",
            "        obj_points, img_points, gray.shape[::-1], None, None\n",
            "    )\n",
            "    \n",
            "    return K, dist\n",
            "\n",
            "def save_calibration(K, dist, output_path):\n",
            "    \"\"\"Save calibration to JSON file.\"\"\"\n",
            "    data = {\n",
            "        \"camera_matrix\": K.tolist(),\n",
            "        \"dist_coeffs\": dist.flatten().tolist()\n",
            "    }\n",
            "    with open(output_path, 'w') as f:\n",
            "        json.dump(data, f, indent=2)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display the calibration tool CLI help\n",
        "import subprocess\n",
        "result = subprocess.run(\n",
        "    ['python3', '../examples/calibrate_camera.py', '--help'],\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "print(result.stdout)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7. References\n",
        "\n",
        "### Papers and Books\n",
        "1. Multiple View Geometry - Hartley and Zisserman (Essential matrix, pose recovery)\n",
        "2. ORB: An efficient alternative to SIFT or SURF - Rublee et al., 2011\n",
        "3. Lucas-Kanade Optical Flow - Lucas and Kanade, 1981\n",
        "\n",
        "### OpenCV Documentation\n",
        "- Feature Detection: https://docs.opencv.org/4.x/db/d27/tutorial_py_table_of_contents_feature2d.html\n",
        "- Camera Calibration: https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html\n",
        "- Pose Estimation: https://docs.opencv.org/4.x/d7/d53/tutorial_py_pose.html\n",
        "\n",
        "### Project Files\n",
        "- PROGRESS.md - Detailed implementation progress\n",
        "- TESTING.md - Testing procedures\n",
        "- TROUBLESHOOTING.md - Common issues and solutions\n",
        "- docs/design_overview.md - Architecture documentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "NETHERGAZE is a fully functional markerless AR pipeline with:\n",
        "\n",
        "‚úÖ **Real-time feature tracking** using ORB + optical flow  \n",
        "‚úÖ **Robust pose estimation** with temporal filtering  \n",
        "‚úÖ **Flexible overlay system** for 2D and 3D content  \n",
        "‚úÖ **CLI-driven application** with extensive configuration  \n",
        "‚úÖ **Camera calibration tool** for accurate intrinsics  \n",
        "‚úÖ **Integration tests** for stability verification  \n",
        "‚úÖ **Sparse SLAM/mapping** with loop closure detection  \n",
        "‚úÖ **Occlusion handling** with depth-aware rendering  \n",
        "\n",
        "**Next steps:**\n",
        "1. Improve scale recovery for metric pose\n",
        "2. Add textured 3D model support\n",
        "3. Integrate dense depth estimation (MiDaS)\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
