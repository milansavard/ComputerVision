{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NETHERGAZE: Markerless Augmented Reality Pipeline\n",
        "\n",
        "**Author:** Milan Savard  \n",
        "**Course:** CS366 F25 Final Personal Project  \n",
        "**Date:** November 2025\n",
        "\n",
        "---\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "NETHERGAZE is a markerless augmented reality (AR) system built using computer vision techniques. Unlike traditional AR systems that rely on fiducial markers (like ArUco or AprilTags), NETHERGAZE uses **natural feature tracking** to estimate camera pose and overlay virtual content.\n",
        "\n",
        "### Key Technologies\n",
        "- **OpenCV** for image processing and computer vision\n",
        "- **ORB (Oriented FAST and Rotated BRIEF)** for feature detection and description\n",
        "- **Lucas-Kanade Optical Flow** for frame-to-frame feature tracking\n",
        "- **Essential Matrix + RANSAC** for robust pose estimation\n",
        "- **Temporal filtering** for smooth AR overlays\n",
        "- **Sparse SLAM** for 3D mapping and loop closure\n",
        "- **Depth-aware rendering** for AR occlusion handling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. [Architecture Overview](#1-architecture-overview)\n",
        "2. [Implementation Progress](#2-implementation-progress)\n",
        "3. [Component Deep Dive](#3-component-deep-dive)\n",
        "4. [Demo: Running the Pipeline](#4-demo-running-the-pipeline)\n",
        "5. [Configuration Options](#5-configuration-options)\n",
        "6. [Project Highlights](#6-project-highlights)\n",
        "7. [Demo Screenshots](#7-demo-screenshots)\n",
        "8. [References](#8-references)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. Architecture Overview\n",
        "\n",
        "The NETHERGAZE pipeline consists of seven main stages:\n",
        "\n",
        "```\n",
        "┌─────────────┐    ┌──────────────┐    ┌────────────────┐    ┌───────────┐    ┌─────────────┐    ┌─────────┐\n",
        "│   Capture   │ -> │   Tracking   │ -> │ Pose Estimation│ -> │  Mapping  │ -> │   Overlay   │ -> │ Display │\n",
        "│  (video.py) │    │ (feature.py) │    │   (pose.py)    │    │(mapping.py│    │ (overlay.py)│    │ (ui.py) │\n",
        "└─────────────┘    └──────────────┘    └────────────────┘    └───────────┘    └─────────────┘    └─────────┘\n",
        "                                                                   │                  │\n",
        "                                                                   v                  v\n",
        "                                                            ┌─────────────┐    ┌─────────────┐\n",
        "                                                            │    Loop     │    │  Occlusion  │\n",
        "                                                            │   Closure   │    │(occlusion.py│\n",
        "                                                            └─────────────┘    └─────────────┘\n",
        "```\n",
        "\n",
        "### Module Responsibilities\n",
        "\n",
        "| Module | File | Purpose |\n",
        "|--------|------|---------|  \n",
        "| **VideoProcessor** | `src/video.py` | Camera capture, preprocessing, backend selection |\n",
        "| **FeatureTracker** | `src/tracking/feature.py` | ORB detection, optical flow, keyframe management |\n",
        "| **PoseEstimator** | `src/pose.py` | Essential matrix, pose recovery, temporal filtering |\n",
        "| **SparseMap** | `src/mapping.py` | 3D point cloud, keyframe management, loop closure |\n",
        "| **OcclusionHandler** | `src/occlusion.py` | Depth estimation, occlusion masks, depth-aware rendering |\n",
        "| **OverlayRenderer** | `src/overlay.py` | 2D/3D overlay rendering, blending |\n",
        "| **UserInterface** | `src/ui.py` | Display window, keyboard controls |\n",
        "| **NETHERGAZEApp** | `src/main.py` | Pipeline orchestration, CLI interface |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Implementation Progress\n",
        "\n",
        "### ✅ All Components Complete\n",
        "\n",
        "| Component | Status | Description |\n",
        "|-----------|--------|-------------|\n",
        "| Video Capture | ✅ Complete | Multi-backend support (AVFoundation, Continuity Camera) |\n",
        "| Feature Tracking | ✅ Complete | ORB + optical flow with keyframe management |\n",
        "| Pose Estimation | ✅ Complete | Essential matrix decomposition with temporal smoothing |\n",
        "| Overlay Rendering | ✅ Complete | 2D primitives + 3D wireframes (cube, pyramid, chair, etc.) |\n",
        "| World Anchoring | ✅ Complete | Objects stay fixed in 3D space as camera moves |\n",
        "| User Interface | ✅ Complete | OpenCV window with keyboard controls and pose stats |\n",
        "| Pipeline Orchestration | ✅ Complete | CLI-driven main application |\n",
        "| Configuration System | ✅ Complete | Default calibration with sensible defaults |\n",
        "| Integration Tests | ✅ Complete | Synthetic video testing, benchmarks, detector comparison |\n",
        "| SLAM/Mapping | ✅ Complete | Sparse 3D mapping with keyframes, triangulation, loop closure |\n",
        "| Occlusion Handling | ✅ Complete | Depth estimation, occlusion masks, depth-aware rendering |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Component Deep Dive\n",
        "\n",
        "Let's explore each component in detail.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /Users/milansavard/Desktop/GitHub/ComputerVision/NETHERGAZE\n",
            "Source path: /Users/milansavard/Desktop/GitHub/ComputerVision/NETHERGAZE/src\n"
          ]
        }
      ],
      "source": [
        "# Setup: Add src to path for imports\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Navigate to project root\n",
        "notebook_dir = os.getcwd()\n",
        "project_root = os.path.dirname(notebook_dir) if 'notebooks' in notebook_dir else notebook_dir\n",
        "src_path = os.path.join(project_root, 'src')\n",
        "sys.path.insert(0, src_path)\n",
        "\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Source path: {src_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Feature Tracking\n",
        "\n",
        "The `FeatureTracker` class implements a hybrid approach:\n",
        "1. **Detection**: ORB features are detected when tracking is lost or features drop below threshold\n",
        "2. **Tracking**: Lucas-Kanade optical flow tracks features frame-to-frame (faster than re-detection)\n",
        "3. **Keyframes**: Best frames are stored for re-localization when tracking fails\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature Tracking Configuration:\n",
            "  Method: orb\n",
            "  Max Features: 1000\n",
            "  Use Optical Flow: True\n",
            "  Optical Flow Window: 21\n",
            "  Reacquire Threshold: 300\n",
            "  Keyframe Interval: 15\n",
            "  Max Keyframes: 5\n"
          ]
        }
      ],
      "source": [
        "from tracking.feature import FeatureTracker, TrackingConfiguration\n",
        "\n",
        "# View default configuration\n",
        "default_config = TrackingConfiguration()\n",
        "print(\"Feature Tracking Configuration:\")\n",
        "print(f\"  Method: {default_config.method}\")\n",
        "print(f\"  Max Features: {default_config.max_features}\")\n",
        "print(f\"  Use Optical Flow: {default_config.use_optical_flow}\")\n",
        "print(f\"  Optical Flow Window: {default_config.optical_flow_win_size}\")\n",
        "print(f\"  Reacquire Threshold: {default_config.reacquire_threshold}\")\n",
        "print(f\"  Keyframe Interval: {default_config.keyframe_interval}\")\n",
        "print(f\"  Max Keyframes: {default_config.max_keyframes}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Pose Estimation\n",
        "\n",
        "The `PoseEstimator` class recovers camera motion from 2D-2D correspondences:\n",
        "\n",
        "1. **Essential Matrix**: Computed from matched points using RANSAC\n",
        "2. **Pose Recovery**: Decompose E into rotation (R) and translation (t)\n",
        "3. **Temporal Filtering**: EMA smoothing + outlier rejection for stable overlays\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pose Filter Configuration:\n",
            "  Enable Smoothing: True\n",
            "  Smoothing Alpha (EMA): 0.3\n",
            "  Enable Outlier Rejection: True\n",
            "  Max Translation Jump: 0.5 m\n",
            "  Max Rotation Jump: 0.5 rad\n",
            "  Min Inliers Threshold: 10\n"
          ]
        }
      ],
      "source": [
        "from pose import PoseEstimator, PoseFilterConfig\n",
        "\n",
        "# View pose filter configuration\n",
        "filter_config = PoseFilterConfig()\n",
        "print(\"Pose Filter Configuration:\")\n",
        "print(f\"  Enable Smoothing: {filter_config.enable_smoothing}\")\n",
        "print(f\"  Smoothing Alpha (EMA): {filter_config.smoothing_alpha}\")\n",
        "print(f\"  Enable Outlier Rejection: {filter_config.enable_outlier_rejection}\")\n",
        "print(f\"  Max Translation Jump: {filter_config.max_translation_jump} m\")\n",
        "print(f\"  Max Rotation Jump: {filter_config.max_rotation_jump} rad\")\n",
        "print(f\"  Min Inliers Threshold: {filter_config.min_inliers_threshold}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Overlay Rendering\n",
        "\n",
        "The `OverlayRenderer` supports both 2D screen-space and 3D world-space overlays:\n",
        "\n",
        "**2D Overlays:**\n",
        "- Text labels\n",
        "- Rectangles, circles, lines\n",
        "- Polygons\n",
        "\n",
        "**3D Wireframe Objects:**\n",
        "- Cube, pyramid, grid\n",
        "- Coordinate axes (RGB = XYZ)\n",
        "- Custom wireframes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overlay Configuration:\n",
            "  Enable 2D Overlays: True\n",
            "  Enable 3D Overlays: True\n",
            "  Default 3D Color: (0, 255, 255) (BGR)\n",
            "  Blend Alpha: 0.7\n",
            "  Antialiasing: True\n"
          ]
        }
      ],
      "source": [
        "from overlay import OverlayRenderer, OverlayConfiguration\n",
        "\n",
        "# View overlay configuration\n",
        "overlay_config = OverlayConfiguration()\n",
        "print(\"Overlay Configuration:\")\n",
        "print(f\"  Enable 2D Overlays: {overlay_config.enable_2d_overlays}\")\n",
        "print(f\"  Enable 3D Overlays: {overlay_config.enable_3d_overlays}\")\n",
        "print(f\"  Default 3D Color: {overlay_config.default_3d_color} (BGR)\")\n",
        "print(f\"  Blend Alpha: {overlay_config.blend_alpha}\")\n",
        "print(f\"  Antialiasing: {overlay_config.antialiasing}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Camera Calibration\n",
        "\n",
        "The system uses a camera intrinsic matrix (K) for projection:\n",
        "\n",
        "```\n",
        "K = | fx   0  cx |\n",
        "    |  0  fy  cy |\n",
        "    |  0   0   1 |\n",
        "```\n",
        "\n",
        "Where:\n",
        "- `fx, fy` = focal lengths in pixels\n",
        "- `cx, cy` = principal point (image center)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Default Camera Matrix (K):\n",
            "[[800.   0. 320.]\n",
            " [  0. 800. 240.]\n",
            " [  0.   0.   1.]]\n",
            "\n",
            "Focal Length: fx=800.0, fy=800.0 pixels\n",
            "Principal Point: cx=320.0, cy=240.0 pixels\n",
            "\n",
            "Distortion Coefficients: [0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "from utils import get_config\n",
        "import numpy as np\n",
        "\n",
        "# Load default calibration\n",
        "config = get_config()\n",
        "calib = config['calibration']\n",
        "\n",
        "K = np.array(calib['camera_matrix'])\n",
        "dist = np.array(calib['dist_coeffs'])\n",
        "\n",
        "print(\"Default Camera Matrix (K):\")\n",
        "print(K)\n",
        "print(f\"\\nFocal Length: fx={K[0,0]:.1f}, fy={K[1,1]:.1f} pixels\")\n",
        "print(f\"Principal Point: cx={K[0,2]:.1f}, cy={K[1,2]:.1f} pixels\")\n",
        "print(f\"\\nDistortion Coefficients: {dist}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Demo: Running the Pipeline\n",
        "\n",
        "### Command Line Usage\n",
        "\n",
        "```bash\n",
        "cd NETHERGAZE\n",
        "\n",
        "# Run the World-Anchored AR Demo\n",
        "python3 src/main.py                    # Default settings\n",
        "python3 src/main.py --fast             # High performance mode (60fps)\n",
        "python3 src/main.py --verbose          # Debug logging\n",
        "\n",
        "# Or run directly from examples\n",
        "python3 examples/demo_anchored_objects.py\n",
        "```\n",
        "\n",
        "### How to Use the AR Demo\n",
        "\n",
        "1. **Point camera** at a textured surface (book, poster, keyboard)\n",
        "2. **Wait for green \"TRACKING\"** indicator in the corner\n",
        "3. **Press SPACE** to set the anchor point (world origin)\n",
        "4. **Press 1-5** to place 3D objects in the scene\n",
        "5. **Move camera around** - objects stay fixed in 3D space!\n",
        "\n",
        "### Keyboard Controls\n",
        "\n",
        "| Key | Action |\n",
        "|-----|--------|\n",
        "| `SPACE` | Set anchor point (world origin) |\n",
        "| `1` | Place wireframe cube |\n",
        "| `2` | Place pyramid |\n",
        "| `3` | Place RGB coordinate axes |\n",
        "| `4` | Place solid box |\n",
        "| `5` | Place 3D chair |\n",
        "| `C` | Clear all placed objects |\n",
        "| `G` | Toggle ground grid |\n",
        "| `M` | Toggle feature markers |\n",
        "| `R` | Reset anchor |\n",
        "| `H` | Toggle help overlay |\n",
        "| `Q` / `ESC` | Quit |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usage: main.py [-h] [--fast] [--verbose]\n",
            "\n",
            "NETHERGAZE - Markerless Augmented Reality with World-Anchored Objects\n",
            "\n",
            "options:\n",
            "  -h, --help     show this help message and exit\n",
            "  --fast, -f     High performance mode (60fps, optimized settings)\n",
            "  --verbose, -V  Enable verbose/debug logging\n",
            "\n",
            "Examples:\n",
            "  python main.py                # Run the AR demo\n",
            "  python main.py --fast         # High performance mode (60fps)\n",
            "  python main.py --verbose      # Enable debug logging\n",
            "\n",
            "Controls (in demo):\n",
            "  SPACE  - Set anchor point (world origin)\n",
            "  1-5    - Place objects (1=cube, 2=pyramid, 3=axes, 4=box, 5=chair)\n",
            "  C      - Clear all objects\n",
            "  G      - Toggle ground grid\n",
            "  M      - Toggle feature markers\n",
            "  R      - Reset anchor\n",
            "  H      - Toggle help overlay\n",
            "  Q      - Quit\n",
            "        \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display the main.py CLI help\n",
        "import subprocess\n",
        "result = subprocess.run(\n",
        "    ['python3', '../src/main.py', '--help'],\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "print(result.stdout)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Configuration Options\n",
        "\n",
        "All parameters can be customized via JSON config file or modified in `src/utils.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full Configuration:\n",
            "{\n",
            "  \"camera_id\": 0,\n",
            "  \"video_width\": 640,\n",
            "  \"video_height\": 480,\n",
            "  \"video_fps\": 30,\n",
            "  \"camera_backend_priority\": null,\n",
            "  \"camera_init_attempts\": 10,\n",
            "  \"enable_preprocessing\": true,\n",
            "  \"blur_kernel\": [\n",
            "    5,\n",
            "    5\n",
            "  ],\n",
            "  \"contrast_alpha\": 1.0,\n",
            "  \"brightness_beta\": 0,\n",
            "  \"axis_length\": 0.05,\n",
            "  \"feature_tracking\": {\n",
            "    \"method\": \"orb\",\n",
            "    \"max_features\": 1000,\n",
            "    \"quality_level\": 0.01,\n",
            "    \"min_distance\": 7.0,\n",
            "    \"fast_threshold\": 20,\n",
            "    \"orb_scale_factor\": 1.2,\n",
            "    \"orb_nlevels\": 8,\n",
            "    \"akaze_threshold\": 0.001,\n",
            "    \"use_optical_flow\": true,\n",
            "    \"optical_flow_win_size\": 21,\n",
            "    \"optical_flow_max_level\": 3,\n",
            "    \"optical_flow_criteria_eps\": 0.03,\n",
            "    \"optical_flow_criteria_count\": 30,\n",
            "    \"adaptive_optical_flow\": true,\n",
            "    \"reacquire_threshold\": 200,\n",
            "    \"keyframe_interval\": 15,\n",
            "    \"max_keyframes\": 6,\n",
            "    \"min_keyframe_features\": 160,\n",
            "    \"keyframe_quality_threshold\": 0.5,\n",
            "    \"matcher_type\": \"bf_hamming\",\n",
            "    \"match_ratio_threshold\": 0.75,\n",
            "    \"use_grid_detection\": false,\n",
            "    \"grid_rows\": 4,\n",
            "    \"grid_cols\": 4\n",
            "  },\n",
            "  \"calibration\": {\n",
            "    \"calibration_file\": null,\n",
            "    \"camera_matrix\": [\n",
            "      [\n",
            "        800.0,\n",
            "        0.0,\n",
            "        320.0\n",
            "      ],\n",
            "      [\n",
            "        0.0,\n",
            "        800.0,\n",
            "        240.0\n",
            "      ],\n",
            "      [\n",
            "        0.0,\n",
            "        0.0,\n",
            "        1.0\n",
            "      ]\n",
            "    ],\n",
            "    \"dist_coeffs\": [\n",
            "      0.0,\n",
            "      0.0,\n",
            "      0.0,\n",
            "      0.0,\n",
            "      0.0\n",
            "    ]\n",
            "  },\n",
            "  \"pose_filter\": {\n",
            "    \"enable_smoothing\": true,\n",
            "    \"smoothing_alpha\": 0.3,\n",
            "    \"enable_outlier_rejection\": true,\n",
            "    \"max_translation_jump\": 0.5,\n",
            "    \"max_rotation_jump\": 0.5,\n",
            "    \"history_size\": 10,\n",
            "    \"use_median_filter\": false,\n",
            "    \"min_inliers_threshold\": 10\n",
            "  },\n",
            "  \"scale_estimation\": {\n",
            "    \"method\": \"auto\",\n",
            "    \"manual_scale\": 1.0,\n",
            "    \"known_distance\": null,\n",
            "    \"ground_plane_height\": 1.5,\n",
            "    \"reference_object_size\": null,\n",
            "    \"scale_smoothing_alpha\": 0.2,\n",
            "    \"min_scale\": 0.001,\n",
            "    \"max_scale\": 100.0,\n",
            "    \"consistency_threshold\": 0.5\n",
            "  },\n",
            "  \"overlay\": {\n",
            "    \"enable_2d_overlays\": true,\n",
            "    \"enable_3d_overlays\": true,\n",
            "    \"default_3d_color\": [\n",
            "      0,\n",
            "      255,\n",
            "      255\n",
            "    ],\n",
            "    \"default_2d_color\": [\n",
            "      0,\n",
            "      255,\n",
            "      0\n",
            "    ],\n",
            "    \"blend_alpha\": 0.7,\n",
            "    \"antialiasing\": true\n",
            "  },\n",
            "  \"display_width\": 640,\n",
            "  \"display_height\": 480,\n",
            "  \"show_markers\": true,\n",
            "  \"show_axes\": true\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from utils import get_config\n",
        "\n",
        "# Get full configuration\n",
        "config = get_config()\n",
        "\n",
        "# Pretty print\n",
        "print(\"Full Configuration:\")\n",
        "print(json.dumps(config, indent=2, default=str))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Configuration Sections\n",
        "\n",
        "| Section | Description |\n",
        "|---------|-------------|\n",
        "| `feature_tracking` | ORB detection and optical flow parameters |\n",
        "| `calibration` | Camera intrinsic matrix and distortion coefficients |\n",
        "| `pose_filter` | Temporal smoothing and outlier rejection settings |\n",
        "| `overlay` | 2D/3D rendering options |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. Project Highlights\n",
        "\n",
        "### Key Features Implemented\n",
        "\n",
        "#### 1. World-Anchored AR Objects\n",
        "Fully implemented in `examples/demo_anchored_objects.py`:\n",
        "- Objects stay fixed in 3D world space as camera moves\n",
        "- Anchor point system for world origin\n",
        "- Multiple object types: cube, pyramid, axes, box, chair\n",
        "- Real-time pose statistics display (FPS, tracking rate, pose rate)\n",
        "\n",
        "```bash\n",
        "# Run the world-anchored AR demo\n",
        "python src/main.py                    # Main entry point\n",
        "python examples/demo_anchored_objects.py  # Direct demo\n",
        "```\n",
        "\n",
        "#### 2. Integration Tests\n",
        "Comprehensive test suite in `tests/test_integration.py`:\n",
        "- Synthetic video generation (checkerboard, feature-rich sequences)\n",
        "- Pipeline metrics collection (tracking rate, pose rate, FPS)\n",
        "- Detector comparison benchmarks (ORB, AKAZE, BRISK)\n",
        "- Video file playback for reproducible testing\n",
        "\n",
        "#### 3. SLAM/Mapping Integration\n",
        "Sparse SLAM in `src/mapping.py`:\n",
        "- `SparseMap` class with 3D point cloud from triangulated features\n",
        "- Keyframe management with covisibility graph\n",
        "- Loop closure detection with geometric verification\n",
        "- Map persistence (save/load to JSON)\n",
        "\n",
        "#### 4. Occlusion Handling\n",
        "Depth-aware rendering in `src/occlusion.py`:\n",
        "- `DepthEstimator` - sparse depth from features + ground plane assumption\n",
        "- `OcclusionHandler` - mask generation and compositing\n",
        "- `DepthAwareOverlayRenderer` - proper AR occlusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ✅ World-Anchored AR Demo\n",
        "\n",
        "The main demo places 3D objects that stay fixed in space. Here's the CLI help:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usage: main.py [-h] [--fast] [--verbose]\n",
            "\n",
            "NETHERGAZE - Markerless Augmented Reality with World-Anchored Objects\n",
            "\n",
            "options:\n",
            "  -h, --help     show this help message and exit\n",
            "  --fast, -f     High performance mode (60fps, optimized settings)\n",
            "  --verbose, -V  Enable verbose/debug logging\n",
            "\n",
            "Examples:\n",
            "  python main.py                # Run the AR demo\n",
            "  python main.py --fast         # High performance mode (60fps)\n",
            "  python main.py --verbose      # Enable debug logging\n",
            "\n",
            "Controls (in demo):\n",
            "  SPACE  - Set anchor point (world origin)\n",
            "  1-5    - Place objects (1=cube, 2=pyramid, 3=axes, 4=box, 5=chair)\n",
            "  C      - Clear all objects\n",
            "  G      - Toggle ground grid\n",
            "  M      - Toggle feature markers\n",
            "  R      - Reset anchor\n",
            "  H      - Toggle help overlay\n",
            "  Q      - Quit\n",
            "        \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display the main.py CLI help\n",
        "import subprocess\n",
        "result = subprocess.run(\n",
        "    ['python3', '../src/main.py', '--help'],\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "print(result.stdout)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7. Demo Screenshots\n",
        "\n",
        "### World-Anchored AR Demo\n",
        "\n",
        "The following screenshots demonstrate NETHERGAZE in action:\n",
        "\n",
        "#### Cube Placement\n",
        "\n",
        "<img src=\"images/demo_cube.png\" alt=\"AR Cube Demo\" width=\"800\"/>\n",
        "\n",
        "*A wireframe cube placed in the scene. Notice the tracking indicators: green feature points, FPS counter, and pose statistics in the corners.*\n",
        "\n",
        "#### UI Elements Visible in Demo\n",
        "- **Top-left**: Help overlay with keyboard controls\n",
        "- **Top-right**: Tracking status (TRACKING/SEARCHING) and anchor status with object count\n",
        "- **Bottom-left**: Real-time statistics (FPS: 30.3, Features: 334, Track: 99%, Pose: 93%)\n",
        "- **Green dots**: ORB features being tracked across the scene (~300+ features for stable pose estimation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "The first attempt did not anchor very well, so I modified the foloowing parameters to increase stability.\n",
        "\n",
        "**max_features** Increased from 2000 to 3000 in order to detect more features\n",
        "\n",
        "**fast_threshold** Decreased from 20 to 10 for more sensitive corner detection\n",
        "\n",
        "**quality_level** Decreased from 0.01 to 0.005 in order to accept weaker features\n",
        "\n",
        "**min_distance** Decreased from 7.0 to 5.0 to allow features closer together\n",
        "\n",
        "**reacquire_threshold** Increased from 300 to 500 to re-detect features sooner\n",
        "\n",
        "**keyframe_interval** Decreased from 10 to 8 for more frequent keyframe updates.\n",
        "\n",
        "**orb_nlevels** Increased from 8 to 12 for more scale pyramid levels\n",
        "\n",
        "<img src=\"images/demo_objects.png\" alt=\"Multiple Objects Demo\" width=\"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 8. References\n",
        "\n",
        "### Papers and Books\n",
        "1. Multiple View Geometry - Hartley and Zisserman (Essential matrix, pose recovery)\n",
        "2. ORB: An efficient alternative to SIFT or SURF - Rublee et al., 2011\n",
        "3. Lucas-Kanade Optical Flow - Lucas and Kanade, 1981\n",
        "\n",
        "### OpenCV Documentation\n",
        "- Feature Detection: https://docs.opencv.org/4.x/db/d27/tutorial_py_table_of_contents_feature2d.html\n",
        "- Camera Calibration: https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html\n",
        "- Pose Estimation: https://docs.opencv.org/4.x/d7/d53/tutorial_py_pose.html\n",
        "\n",
        "### Project Files\n",
        "- PROGRESS.md - Detailed implementation progress\n",
        "- TESTING.md - Testing procedures\n",
        "- TROUBLESHOOTING.md - Common issues and solutions\n",
        "- docs/design_overview.md - Architecture documentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "NETHERGAZE is a fully functional markerless AR pipeline with:\n",
        "\n",
        "✅ **Real-time feature tracking** using ORB + optical flow  \n",
        "✅ **Robust pose estimation** with temporal filtering  \n",
        "✅ **World-anchored 3D objects** that stay fixed in space  \n",
        "✅ **Multiple 3D object types** (cube, pyramid, axes, box, chair)  \n",
        "✅ **Live pose statistics** (FPS, tracking rate, pose rate, feature count)  \n",
        "✅ **Integration tests** for stability verification  \n",
        "✅ **Sparse SLAM/mapping** with loop closure detection  \n",
        "✅ **Occlusion handling** with depth-aware rendering  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
